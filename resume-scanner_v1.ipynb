{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-groq==0.2.4 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (0.2.4)\n",
      "Requirement already satisfied: pypdf==5.3.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (5.3.0)\n",
      "Requirement already satisfied: langchain-core==0.3.37 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (0.3.37)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: langfuse==2.59.4 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (2.59.4)\n",
      "Requirement already satisfied: langchain==0.3.19 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.3.19)\n",
      "Collecting streamlit==1.42.2 (from -r requirements.txt (line 7))\n",
      "  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting python-docx==1.1.2 (from -r requirements.txt (line 8))\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain-groq==0.2.4->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain-core==0.3.37->-r requirements.txt (line 3)) (0.3.10)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain-core==0.3.37->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain-core==0.3.37->-r requirements.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain-core==0.3.37->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain-core==0.3.37->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain-core==0.3.37->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain-core==0.3.37->-r requirements.txt (line 3)) (2.10.6)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.4.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langfuse==2.59.4->-r requirements.txt (line 5)) (4.8.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langfuse==2.59.4->-r requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langfuse==2.59.4->-r requirements.txt (line 5)) (0.28.1)\n",
      "Requirement already satisfied: idna<4.0,>=3.7 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langfuse==2.59.4->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langfuse==2.59.4->-r requirements.txt (line 5)) (2.32.3)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langfuse==2.59.4->-r requirements.txt (line 5)) (1.17.2)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain==0.3.19->-r requirements.txt (line 6)) (0.3.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain==0.3.19->-r requirements.txt (line 6)) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain==0.3.19->-r requirements.txt (line 6)) (3.11.13)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langchain==0.3.19->-r requirements.txt (line 6)) (2.2.3)\n",
      "Collecting altair<6,>=4.0 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pandas<3,>=1.4.0 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from streamlit==1.42.2->-r requirements.txt (line 7)) (11.1.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from streamlit==1.42.2->-r requirements.txt (line 7)) (6.4.2)\n",
      "Collecting lxml>=3.1.0 (from python-docx==1.1.2->-r requirements.txt (line 8))\n",
      "  Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19->-r requirements.txt (line 6)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19->-r requirements.txt (line 6)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19->-r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19->-r requirements.txt (line 6)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19->-r requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.3.19->-r requirements.txt (line 6)) (1.18.3)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading narwhals-1.28.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from anyio<5.0.0,>=4.4.0->langfuse==2.59.4->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: colorama in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit==1.42.2->-r requirements.txt (line 7)) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq==0.2.4->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: certifi in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse==2.59.4->-r requirements.txt (line 5)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from httpx<1.0,>=0.15.4->langfuse==2.59.4->-r requirements.txt (line 5)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==2.59.4->-r requirements.txt (line 5)) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.37->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.37->-r requirements.txt (line 3)) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.37->-r requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core==0.3.37->-r requirements.txt (line 3)) (0.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit==1.42.2->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3,>=1.4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3,>=1.4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core==0.3.37->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core==0.3.37->-r requirements.txt (line 3)) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from requests<3,>=2->langfuse==2.59.4->-r requirements.txt (line 5)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from requests<3,>=2->langfuse==2.59.4->-r requirements.txt (line 5)) (2.3.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit==1.42.2->-r requirements.txt (line 7)) (2.19.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.19->-r requirements.txt (line 6)) (3.1.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading rpds_py-0.23.1-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.42.2->-r requirements.txt (line 7))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit==1.42.2->-r requirements.txt (line 7)) (1.17.0)\n",
      "Downloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.8/9.6 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.6 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.6 MB 8.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.6/9.6 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.6 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.4/9.6 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.6/9.6 MB 7.0 MB/s eta 0:00:00\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 731.2/731.2 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 8.3 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 6.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.5 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/25.3 MB 5.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 2.9/25.3 MB 7.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.2/25.3 MB 7.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.0/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.9/25.3 MB 7.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.4/25.3 MB 7.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.5/25.3 MB 8.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.4/25.3 MB 8.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.5/25.3 MB 8.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.3/25.3 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.4/25.3 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.5/25.3 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.6/25.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 7.6 MB/s eta 0:00:00\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.8/6.9 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.7/6.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/6.9 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading narwhals-1.28.0-py3-none-any.whl (308 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.23.1-cp312-cp312-win_amd64.whl (237 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, watchdog, tzdata, toml, smmap, rpds-py, pyarrow, protobuf, narwhals, mdurl, MarkupSafe, lxml, click, cachetools, blinker, referencing, python-docx, pandas, markdown-it-py, jinja2, gitdb, rich, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed MarkupSafe-3.0.2 altair-5.5.0 blinker-1.9.0 cachetools-5.5.2 click-8.1.8 gitdb-4.0.12 gitpython-3.1.44 jinja2-3.1.5 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 lxml-5.3.1 markdown-it-py-3.0.0 mdurl-0.1.2 narwhals-1.28.0 pandas-2.2.3 protobuf-5.29.3 pyarrow-19.0.1 pydeck-0.9.1 python-docx-1.1.2 pytz-2025.1 referencing-0.36.2 rich-13.9.4 rpds-py-0.23.1 smmap-5.0.2 streamlit-1.42.2 toml-0.10.2 tzdata-2025.1 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4==4.13.3\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4==4.13.3)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from beautifulsoup4==4.13.3) (4.12.2)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.3 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4==4.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: 403 Client Error: Forbidden for url: https://in.indeed.com/viewjob?jk=f1c8238b13579948&from=shareddesktop_copy'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_indeed_job(url):\n",
    "    \"\"\"\n",
    "    Scrape job descriptions from Indeed.\n",
    "    Indeed is simpler to scrape than LinkedIn but still has some protections.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find the job description container\n",
    "        job_description = soup.find(\"div\", id=\"jobDescriptionText\")\n",
    "        \n",
    "        if job_description:\n",
    "            print(job_description.get_text(separator=\"\\n\").strip())\n",
    "            return job_description.get_text(separator=\"\\n\").strip()\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def scrape_glassdoor_job(url):\n",
    "    \"\"\"\n",
    "    Scrape job descriptions from Glassdoor.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find the job description\n",
    "        job_description = soup.find(\"div\", class_=\"jobDescriptionContent\")\n",
    "        \n",
    "        if job_description:\n",
    "            print(job_description.get_text(separator=\"\\n\").strip())\n",
    "            return job_description.get_text(separator=\"\\n\").strip()\n",
    "            \n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "scrape_indeed_job(\"https://in.indeed.com/viewjob?jk=f1c8238b13579948&from=shareddesktop_copy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002E374BE2390>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/google-api-python-client/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002E374BBA570>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/google-api-python-client/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002E374F2C920>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/google-api-python-client/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002E374F2CB30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/google-api-python-client/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002E374F2CD10>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/google-api-python-client/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading google_api_python_client-2.162.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading googleapis_common_protos-1.69.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.29.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading proto_plus-1.26.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.2)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client)\n",
      "  Downloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\resumeradar – ai-driven candidate screening\\.venv\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
      "Downloading google_api_python_client-2.162.0-py2.py3-none-any.whl (13.1 MB)\n",
      "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.1 MB 1.0 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/13.1 MB 1.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.0/13.1 MB 1.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 1.0/13.1 MB 1.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/13.1 MB 907.1 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/13.1 MB 907.1 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/13.1 MB 907.1 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.3/13.1 MB 907.1 kB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 1.6/13.1 MB 640.3 kB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 1.8/13.1 MB 662.3 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.1/13.1 MB 711.8 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 2.4/13.1 MB 745.8 kB/s eta 0:00:15\n",
      "   ------- -------------------------------- 2.4/13.1 MB 745.8 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 2.6/13.1 MB 766.5 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.6/13.1 MB 766.5 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.9/13.1 MB 745.8 kB/s eta 0:00:14\n",
      "   --------- ------------------------------ 3.1/13.1 MB 782.1 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 3.4/13.1 MB 783.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 3.7/13.1 MB 796.0 kB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 3.7/13.1 MB 796.0 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.9/13.1 MB 780.4 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.9/13.1 MB 780.4 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 4.2/13.1 MB 779.1 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 4.2/13.1 MB 779.1 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 4.5/13.1 MB 782.7 kB/s eta 0:00:12\n",
      "   --------------- ------------------------ 5.0/13.1 MB 820.6 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 5.0/13.1 MB 820.6 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 5.2/13.1 MB 830.1 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 5.5/13.1 MB 843.0 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 5.8/13.1 MB 859.4 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 6.0/13.1 MB 866.5 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 6.3/13.1 MB 863.3 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 6.6/13.1 MB 875.4 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 6.6/13.1 MB 875.4 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 6.8/13.1 MB 868.4 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 7.1/13.1 MB 865.5 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 7.1/13.1 MB 865.5 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 7.1/13.1 MB 865.5 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 7.1/13.1 MB 865.5 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 7.3/13.1 MB 813.3 kB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 7.3/13.1 MB 813.3 kB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 7.6/13.1 MB 810.0 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 7.9/13.1 MB 819.1 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 8.1/13.1 MB 827.8 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 8.4/13.1 MB 834.9 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 8.7/13.1 MB 845.5 kB/s eta 0:00:06\n",
      "   --------------------------- ------------ 8.9/13.1 MB 858.4 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 9.2/13.1 MB 865.6 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 9.4/13.1 MB 872.6 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 9.4/13.1 MB 872.6 kB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 9.7/13.1 MB 864.1 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 10.0/13.1 MB 865.8 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 10.2/13.1 MB 874.6 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 10.5/13.1 MB 874.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.5/13.1 MB 874.8 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.7/13.1 MB 877.2 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.3/13.1 MB 894.3 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 11.3/13.1 MB 894.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 11.5/13.1 MB 895.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.5/13.1 MB 895.0 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.8/13.1 MB 880.9 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.1/13.1 MB 886.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 12.3/13.1 MB 890.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.3/13.1 MB 890.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.6/13.1 MB 884.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/13.1 MB 885.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/13.1 MB 885.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.1/13.1 MB 878.7 kB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.69.0-py2.py3-none-any.whl (169 kB)\n",
      "Downloading proto_plus-1.26.0-py3-none-any.whl (50 kB)\n",
      "Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, proto-plus, googleapis-common-protos, rsa, pyasn1-modules, httplib2, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed google-api-core-2.24.1 google-api-python-client-2.162.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.69.0 httplib2-0.22.0 proto-plus-1.26.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyparsing-3.2.1 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job description not found.\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "def fetch_job_description_with_google_cse(job_title, site=\"accenture.com\"):\n",
    "    try:\n",
    "        # Google Custom Search API credentials\n",
    "        api_key = \"your_google_api_key\"\n",
    "        search_engine_id = \"your_search_engine_id\"\n",
    "\n",
    "        # Build the service\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "\n",
    "        # Perform the search\n",
    "        query = f\"{job_title} site:{\"https://in.indeed.com/viewjob?jk=f1c8238b13579948&from=shareddesktop_copy\"}\"\n",
    "        result = service.cse().list(q=query, cx=search_engine_id).execute()\n",
    "\n",
    "        # Extract the job description URL\n",
    "        if \"items\" in result:\n",
    "            job_url = result[\"items\"][0][\"link\"]  # Get the first result URL\n",
    "            return job_url\n",
    "        else:\n",
    "            print(\"No job postings found.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching job description: {e}\")\n",
    "        return None\n",
    "\n",
    "fetch_job_description_with_google_cse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping job description: 403 Client Error: Forbidden for url: https://in.indeed.com/viewjob?jk=f1c8238b13579948&from=shareddesktop_copy\n",
      "Failed to scrape job description.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_indeed_job_description(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract the job description\n",
    "        job_description_div = soup.find(\"div\", id=\"jobDescriptionText\")\n",
    "        if job_description_div:\n",
    "            # Find all <p> tags inside the job description div\n",
    "            paragraphs = job_description_div.find_all(\"p\")\n",
    "            job_description = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "            return job_description\n",
    "        else:\n",
    "            print(\"Job description not found on the page.\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping job description: {e}\")\n",
    "        return None\n",
    "url = \"https://in.indeed.com/viewjob?jk=f1c8238b13579948&from=shareddesktop_copy\"\n",
    "job_description = scrape_indeed_job_description(url)\n",
    "if job_description:\n",
    "    print(job_description)\n",
    "else:\n",
    "    print(\"Failed to scrape job description.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java Full Stack Developers - Tricon Infotech\n",
      "Scroll to top\n",
      "Light\n",
      "Dark\n",
      "Light\n",
      "Dark\n",
      "Follow Us\n",
      "Skip to content\n",
      "Solutions\n",
      "Industries\n",
      "Insights\n",
      "Difference\n",
      "Careers\n",
      "About\n",
      "Solutions\n",
      "Industries\n",
      "Insights\n",
      "Difference\n",
      "Careers\n",
      "About\n",
      "Get In Touch\n",
      "221 River Street, 9th Floor,\n",
      "Hoboken, New Jersey 07030\n",
      "United States of America\n",
      "Terms of Use\n",
      "Privacy Policy\n",
      "Cookies\n",
      "Image Credits\n",
      "Contact Us\n",
      "Java Full Stack Developers\n",
      "Job Type:\n",
      "Full Time\n",
      "Job Location:\n",
      "Bangalore\n",
      "Skills/Requirements:\n",
      "Experience Required : 3 to 8 Years – Different Roles\n",
      "Experience in software industry on Java J2EE Technologies.\n",
      "Experience in Javascript Frameworks – Angular/AngularJs/NodeJs\n",
      "Involved in designing and developing solutions for complex systems.\n",
      "Good command over OOPS Concepts, Design Patterns.\n",
      "Good experience on Databases (SQL and NoSQL)\n",
      "Worked on performance tuning at Application and Database layers.\n",
      "Worked on micro-services and EDA bases architecture (AWS cloud and services)\n",
      "Analyzing requirement documents & wireframes for the feasibility & implementation of the features.\n",
      "Good experience on Application development (Web/Rest/Scheduler based applications).\n",
      "Analyzed technical requirements and framework evaluation for implementations.\n",
      "Integrated Applications with monitoring tools (New Relic)\n",
      "Experience working with Agile methodologies and have a TDD and BDD approach to software development and familiarity with testing frameworks.\n",
      "Cloud Exposure is an added advantage\n",
      "Apply for this position\n",
      "Full Name\n",
      "*\n",
      "Email\n",
      "*\n",
      "Phone\n",
      "*\n",
      "Cover Letter\n",
      "*\n",
      "Upload CV/Resume\n",
      "*\n",
      "Allowed Type(s): .pdf, .doc, .docx, .rtf\n",
      "Find your solution\n",
      "Contact us\n",
      "Begin your journey\n",
      "Name (required)\n",
      "Company Email (required)\n",
      "Phone\n",
      "Message\n",
      "Copyright\n",
      "©\n",
      "2024 Tricon Infotech. All Rights Reserved.\n",
      "Terms of Use\n",
      "Privacy Policy\n",
      "Cookies\n",
      "Image Credits\n",
      "Copyright\n",
      "©\n",
      "2024 Tricon Infotech. All Rights Reserved.\n",
      "Follow Us\n",
      "—\n",
      "Copyright\n",
      "©\n",
      "2024 Tricon Infotech. All Rights Reserved.\n",
      "Follow Us\n",
      "—\n",
      "twitter\n",
      "facebook\n",
      "linkedin\n",
      "Manage Cookie Consent\n",
      "To provide the best experiences, we use technologies like cookies to store and/or access device information. Consenting to these technologies will allow us to process data such as browsing behavior or unique IDs on this site. Not consenting or withdrawing consent, may adversely affect certain features and functions.\n",
      "Functional\n",
      "Functional\n",
      "Always active\n",
      "The technical storage or access is strictly necessary for the legitimate purpose of enabling the use of a specific service explicitly requested by the subscriber or user, or for the sole purpose of carrying out the transmission of a communication over an electronic communications network.\n",
      "Preferences\n",
      "Preferences\n",
      "The technical storage or access is necessary for the legitimate purpose of storing preferences that are not requested by the subscriber or user.\n",
      "Statistics\n",
      "Statistics\n",
      "The technical storage or access that is used exclusively for statistical purposes.\n",
      "The technical storage or access that is used exclusively for anonymous statistical purposes. Without a subpoena, voluntary compliance on the part of your Internet Service Provider, or additional records from a third party, information stored or retrieved for this purpose alone cannot usually be used to identify you.\n",
      "Marketing\n",
      "Marketing\n",
      "The technical storage or access is required to create user profiles to send advertising, or to track the user on a website or across several websites for similar marketing purposes.\n",
      "Manage options\n",
      "Manage services\n",
      "Manage {vendor_count} vendors\n",
      "Read more about these purposes\n",
      "Accept\n",
      "Deny\n",
      "View preferences\n",
      "Save preferences\n",
      "View preferences\n",
      "{title}\n",
      "{title}\n",
      "{title}\n",
      "Manage consent\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.triconinfotech.com/jobs/java-full-stack-developers/\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extracting all text content\n",
    "    job_description = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    \n",
    "    print(job_description)\n",
    "else:\n",
    "    print(f\"Failed to fetch page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job description not found.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Replace with a valid job URL from the CSE API results\n",
    "job_url = \"https://in.indeed.com/viewjob?jk=f1c8238b13579948&from=shareddesktop_copy\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "response = requests.get(job_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# 🔍 Find job description div (adjust selector if needed)\n",
    "job_description = soup.find(\"div\", class_=\"jobsearch-jobDescriptionText\")\n",
    "if job_description:\n",
    "    print(job_description.text.strip())\n",
    "else:\n",
    "    print(\"Job description not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\n",
      "Downloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n",
      "Installing collected packages: fake-useragent\n",
      "Successfully installed fake-useragent-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Failed to access page. Status code: 403'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "def scrape_simplyhired_job(url):\n",
    "    try:\n",
    "        ua = UserAgent()\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": ua.random,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Find the job description\n",
    "            job_description = soup.find(\"div\", class_=\"viewjob-description\")\n",
    "            \n",
    "            if not job_description:\n",
    "                job_description = soup.find(\"div\", class_=\"ViewJobDescription-description\")\n",
    "                \n",
    "            if job_description:\n",
    "                return job_description.get_text(separator=\"\\n\").strip()\n",
    "            else:\n",
    "                return \"Job description element not found. The site may have changed its structure.\"\n",
    "        else:\n",
    "            return f\"Failed to access page. Status code: {response.status_code}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "scrape_simplyhired_job(\"https://www.simplyhired.co.in/job/Ah56pSxcUzQyg_XZziLxdhN9WqAW3m_aj2kNjX2KBmxk34EGk2En8w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Job Description\n",
    "job_description = \"\"\"\n",
    "Job Description:\n",
    "Job Title: Generative AI Developer\n",
    "\n",
    "Job Location: Singapore\n",
    "\n",
    "Job Summary:\n",
    "\n",
    "We are looking for a highly skilled and experienced Generative AI Developer to join our team. The ideal candidate will have a strong understanding of generative AI Models, Frameoworks, techniques and be able to apply them to develop innovative solutions. The successful candidate will also have a proven track record of working in cloud computing environments and be able to deploy and manage machine learning models in the cloud.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Design, develop, and implement generative AI models using state-of-the-art techniques. Collaborate with cross-functional teams to define project goals, research requirements, and develop innovative solutions. Optimize model performance through experimentation, hyperparameter tuning, and advanced optimization techniques. Stay up-to-date on the latest advancements in generative AI, deep learning, and related fields, and incorporate new techniques and methods into the team's workflow. Develop and maintain clear and concise documentation of generative AI models, processes, and results.\n",
    "\n",
    "Communicate complex concepts and results to both technical and non-technical stakeholders. Provide support and guidance to other team members, and contribute to a positive, collaborative working environment.\n",
    "\n",
    "Qualifications:\n",
    "\n",
    "Bachelor's degree in computer science, artificial intelligence, or a related field.\n",
    "\n",
    "Few years of experience in generative AI development.\n",
    "\n",
    "Strong programming skills in Python/Java.\n",
    "\n",
    "Experience with machine learning frameworks, such as TensorFlow, PyTorch, or\n",
    "\n",
    "scikit-learn.\n",
    "\n",
    "Experience with cloud computing platforms, such as Google Cloud Platform (GCP).\n",
    "\n",
    "Excellent communication and problem-solving skills.\n",
    "\n",
    "Ability to work independently and as part of a team.\n",
    "\n",
    "Must Have Skills Technical Skills\n",
    "\n",
    "Programming languages: Python, Java, or C++\n",
    "\n",
    "Machine learning frameworks: TensorFlow, PyTorch, or scikit-learn\n",
    "\n",
    "Cloud computing platforms: Google Cloud Platform (GCP)\n",
    "\n",
    "Natural language processing (NLP)\n",
    "\n",
    "Generative adversarial networks (GANs)\n",
    "\n",
    "Variational autoencoders (VAEs)\n",
    "\n",
    "Diffusion models\n",
    "\n",
    "Data visualization\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results for 0491608c-dfd3-436e-b1f7-77ecb2f89f3c:\n",
      "Job ID: 001\n",
      "Candidate ID: 0491608c-dfd3-436e-b1f7-77ecb2f89f3c\n",
      "\n",
      "### Job Alignment Assessment:\n",
      "\n",
      "#### 1. **Core Competency Match** 🔍\n",
      "\n",
      "- **Skills from the JD that are clearly demonstrated in the resume:**\n",
      "  - Python (JD: Python/Java, Resume: Python)\n",
      "  - Generative AI (JD: Generative AI, Resume: Generative AI)\n",
      "  - AWS (JD: Cloud computing platforms, Resume: AWS - Basics)\n",
      "  - FastAPI (JD: Machine learning frameworks, Resume: FastAPI)\n",
      "  - LangGraph (JD: Machine learning frameworks, Resume: intelligent agent orchestration using LangGraph)\n",
      "  - Angular (JD: Not mentioned, Resume: Angular)\n",
      "  - Structured Query Language (JD: Not mentioned, Resume: Structured Query Language)\n",
      "  - Circle CI (JD: Not mentioned, Resume: Circle CI)\n",
      "  - Data visualization (JD: Data visualization, Resume: Not explicitly mentioned, but implied through various projects)\n",
      "\n",
      "- **Missing critical requirements from the JD:**\n",
      "  - Java (JD: Python/Java, Resume: Python)\n",
      "  - TensorFlow, PyTorch, or scikit-learn (JD: Machine learning frameworks, Resume: FastAPI and LangGraph)\n",
      "  - Google Cloud Platform (GCP) (JD: Cloud computing platforms, Resume: AWS - Basics)\n",
      "  - Natural language processing (NLP) (JD: Natural language processing, Resume: Not explicitly mentioned)\n",
      "  - Generative adversarial networks (GANs) (JD: Generative adversarial networks, Resume: Not explicitly mentioned)\n",
      "  - Variational autoencoders (VAEs) (JD: Variational autoencoders, Resume: Not explicitly mentioned)\n",
      "  - Diffusion models (JD: Diffusion models, Resume: Not explicitly mentioned)\n",
      "\n",
      "- **Direct quotes from both documents for comparison:**\n",
      "  - JD: \"Design, develop, and implement generative AI models using state-of-the-art techniques.\"\n",
      "  - Resume: \"Implemented enhancements to an AI-enabled employee portal, introducing support for tables, images, graphs, and text data for Retrieval Augmented Generation tasks...\"\n",
      "\n",
      "#### 2. **Experience Evaluation** 🔍\n",
      "\n",
      "- **Years of experience in required technologies/domains:**\n",
      "  - Generative AI: 1-2 years (Resume: Implemented enhancements to an AI-enabled employee portal, introduced support for tables, images, graphs, and text data for Retrieval Augmented Generation tasks)\n",
      "  - Cloud computing platforms: 1-2 years (Resume: Migrated from GPT to AWS-hosted models)\n",
      "  - Machine learning frameworks: 1-2 years (Resume: Implemented multi-vector retrieval architecture, used LangGraph for intelligent agent orchestration)\n",
      "\n",
      "- **Relevance of ALL previous roles to the target position:**\n",
      "  - Tricon Infotech: Relevant (Implemented enhancements to an AI-enabled employee portal, introduced support for tables, images, graphs, and text data for Retrieval Augmented Generation tasks)\n",
      "  - Tata Consultancy Services: Relevant (Built a forecasting system using ARIMA and Prophet time-series models)\n",
      "\n",
      "- **Gaps in employment history:**\n",
      "  - None\n",
      "\n",
      "- **Career progression:**\n",
      "  - Candidate has progressed from an internship at Tata Consultancy Services to a software engineer at Tricon Infotech\n",
      "\n",
      "#### 3. **Achievement Analysis** 🔍\n",
      "\n",
      "- **Quantified accomplishments that align with JD priorities:**\n",
      "  - \"Significantly reducing operational costs by migrating from GPT to AWS-hosted models, resulting in a 95% decrease in input token costs and an 87.5% reduction in output token costs.\"\n",
      "  - \"Achieving 40% faster query response times and 35% improvement in search result relevance.\"\n",
      "  - \"Improving operational efficiency by 45% through intelligent agent orchestration using LangGraph.\"\n",
      "\n",
      "- **Missing metrics that could strengthen the resume:**\n",
      "  - Quantified metrics for the following accomplishments:\n",
      "    - \"Collaborated with a cross-functional team to develop new Angular UI for a learning management platform, improving search experience for educators and students.\"\n",
      "    - \"Developed high-performance FastAPIs with Uvicorn and asynchronous request handling, optimizing API responsiveness and system scalability.\"\n",
      "    - \"Implemented intelligent agent orchestration using LangGraph, automating agent selection and routing processes, significantly reducing manual user interactions while improving operational efficiency by 45%.\"\n",
      "\n",
      "### Format Assessment: 🔍\n",
      "\n",
      "- **ATS Readability Score (1-5):** 4/5\n",
      "  - The resume is well-structured and easy to read, but some sections (e.g., skills) could be improved for better readability.\n",
      "\n",
      "- **Keyword Optimization against JD:** 3.5/5\n",
      "  - The resume includes some relevant keywords from the JD, but could benefit from more optimization to match the job description.\n",
      "\n",
      "- **Visual presentation analysis:**\n",
      "  - The resume has a clean and modern design, with clear headings and bullet points.\n",
      "\n",
      "### Overall Rating:\n",
      "\n",
      "- **Final rating on a scale of 0-5 (with 5 being perfect match):** 3.5/5\n",
      "- **Overall percentage match score (percentage alignment):** 72%\n",
      "\n",
      "### Final Summary:\n",
      "\n",
      "- **Strongest qualifications for this role:**\n",
      "  - Experience with generative AI and cloud computing platforms\n",
      "  - Strong programming skills in Python\n",
      "  - Ability to optimize model performance through experimentation and hyperparameter tuning\n",
      "\n",
      "- **Most significant potential drawbacks:**\n",
      "  - Lack of experience with Java, TensorFlow, PyTorch, or scikit-learn\n",
      "  - Limited experience with Google Cloud Platform (GCP)\n",
      "  - No explicit mention of natural language processing (NLP), generative adversarial networks (GANs), variational autoencoders (VAEs), or diffusion models\n",
      "\n",
      "- **Recommended interview focus areas:**\n",
      "  - Generative AI development and deployment\n",
      "  - Cloud computing platforms and machine learning frameworks\n",
      "  - Optimization techniques and experimentation\n",
      "  - Communication and problem-solving skills\n"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "    secret_key=os.environ.get(\"LANGFUSE_SECRET_KEY\"),\n",
    "    public_key=os.environ.get(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    host=os.environ.get(\"LANGFUSE_HOST\"),\n",
    ")\n",
    "\n",
    "langfuse_handler.auth_check()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyPDF.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    \"\"\"Extract text from a DOCX file.\"\"\"\n",
    "    return docx2txt.process(docx_path)\n",
    "\n",
    "def extract_text_from_resume(file_path, candidate_id):\n",
    "    \"\"\"Extract text from a resume (PDF or DOCX) and append candidate_id.\"\"\"\n",
    "    if file_path.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please upload a PDF or DOCX file.\")\n",
    "    \n",
    "    # Append candidate_id to the extracted text\n",
    "    text_with_candidate_id = f\"Candidate ID: {candidate_id}\\n\\n{text}\"\n",
    "    return text_with_candidate_id\n",
    "\n",
    "def evaluate_resume(resume_text, job_description, candidate_id, job_id):\n",
    "    \"\"\"Evaluate the resume using the LLM and append job_id and candidate_id to the response.\"\"\"\n",
    "    # Combine the resume and job description into a prompt\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    As a Senior Talent Acquisition Specialist in technical recruiting, conduct a comprehensive analysis of this resume against the provided job description. Provide a structured evaluation using the following framework:\n",
    "\n",
    "    ### Job Alignment Assessment:\n",
    "    1. **Core Competency Match** 🔍\n",
    "    - Identify ALL skills from the JD that are clearly demonstrated in the resume\n",
    "    - Highlight any missing critical requirements from the JD\n",
    "    - Include direct quotes from both documents for comparison\n",
    "\n",
    "    2. **Experience Evaluation** 🔍\n",
    "    - Compare years of experience in required technologies/domains\n",
    "    - Analyze relevance of ALL previous roles to the target position\n",
    "    - Note any gaps in employment history\n",
    "    - Analyze career progression\n",
    "\n",
    "    3. **Achievement Analysis** 🔍\n",
    "    - Flag ALL quantified accomplishments that align with JD priorities\n",
    "    - Identify missing metrics that could strengthen the resume\n",
    "\n",
    "    ### Format Assessment: 🔍\n",
    "    - ATS Readability Score (1-5)\n",
    "    - Keyword Optimization against JD\n",
    "    - Visual presentation analysis\n",
    "\n",
    "    ### Overall Rating:\n",
    "    - Provide a final rating on a scale of 0-5 (with 5 being perfect match)\n",
    "    - Calculate an overall percentage match score (percentage alignment) \n",
    "\n",
    "    ### Final Summary:\n",
    "    - Strongest qualifications for this role\n",
    "    - Most significant potential drawbacks\n",
    "    - Recommended interview focus areas\n",
    "\n",
    "    Resume:\n",
    "    {resume_text}\n",
    "\n",
    "    Job Description: \n",
    "    {job_description} \n",
    "\"\"\"\n",
    ")\n",
    "    \n",
    "    model = ChatGroq(temperature=0.2, groq_api_key=groq_api_key, model_name=\"llama-3.1-8b-instant\")\n",
    "\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    evaluation = chain.invoke({\n",
    "        \"resume_text\": resume_text,\n",
    "        \"job_description\": job_description\n",
    "    }, config={\"callbacks\":[langfuse_handler]})\n",
    "\n",
    "    # Append job_id and candidate_id to the evaluation response\n",
    "    evaluation_with_ids = f\"Job ID: {job_id}\\nCandidate ID: {candidate_id}\\n\\n{evaluation}\"\n",
    "    return evaluation_with_ids\n",
    "\n",
    "# Example usage\n",
    "file_path = r\"D:\\Kedar Santosh Prabhu-Resume (1).pdf\"\n",
    "job_description = job_description \n",
    "job_id = \"001\" \n",
    "candidate_id = str(uuid.uuid4())  # Generate a UUID for candidate_id\n",
    "\n",
    "# Extract text and append candidate_id\n",
    "resume_text_with_candidate_id = extract_text_from_resume(file_path, candidate_id)\n",
    "\n",
    "# Evaluate the resume and append job_id and candidate_id\n",
    "evaluation = evaluate_resume(resume_text_with_candidate_id, job_description, candidate_id, job_id)\n",
    "\n",
    "print(f\"Evaluation results for {candidate_id}:\")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Job Description\\nWe are looking for a talented Software Engineer with expertise in Generative AI, full-stack development, and cloud technologies to join our cutting-edge team. The ideal candidate will have a strong background in Python, AI/ML technologies, and web development frameworks.\\nKey Responsibilities\\n\\nDesign and implement AI-enabled enterprise platforms with advanced information retrieval capabilities\\nDevelop high-performance APIs using FastAPI and asynchronous programming techniques\\nCreate scalable web applications using Angular and modern front-end technologies\\nImplement intelligent agent orchestration and retrieval augmented generation (RAG) systems\\nOptimize cloud-based AI model deployments to reduce operational costs\\nCollaborate with cross-functional teams to deliver innovative software solutions\\n\\nRequired Technical Skills\\n\\nProficiency in Python\\nExperience with Generative AI technologies\\nStrong understanding of machine learning concepts\\nFull-stack development skills (Angular, TypeScript, HTML, CSS)\\nAPI development using FastAPI\\nBasic AWS cloud infrastructure knowledge\\nFamiliarity with CI/CD pipelines (CircleCI)\\nExperience with asynchronous programming\\n\\nPreferred Qualifications\\n\\nBachelor's degree in Computer Science, Information Science, or related field\\nExperience with:\\n\\nLangGraph for agent orchestration\\nMulti-vector retrieval architectures\\nTime-series forecasting models\\nCloud AI model deployment and optimization\\nCertifications in AI and cloud technologies\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg, os, uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Database credentials\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "\n",
    "def get_job_description(job_id):\n",
    "    \"\"\"Fetch job description by job_id.\"\"\"\n",
    "    conn_string = f\"host={DB_HOST} dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} port={DB_PORT}\"\n",
    "    \n",
    "    try:\n",
    "        job_id = uuid.UUID(job_id)\n",
    "    except ValueError:\n",
    "        print(\"Invalid Job ID format. Please enter a valid UUID.\")\n",
    "        return None\n",
    "\n",
    "    with psycopg.connect(conn_string) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT job_description FROM job_descriptions WHERE job_id = %s\", (job_id,))\n",
    "            result = cursor.fetchone()\n",
    "            return result[0] if result else None\n",
    "        \n",
    "get_job_description('a49181cc-39a7-4308-877e-bf9c97e4af8c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 10:21:21.890 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.530 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run d:\\ResumeRadar – AI-driven candidate screening\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-30 10:21:22.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Session state does not function when running a script without `streamlit run`\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-30 10:21:22.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(UUID('9b2619cc-7b07-4db9-8d87-44bb9737f750'), '  \\nKEDAR SANTOSH PRABHU \\nkedarprabhu2000@gmail.com · +91-8310629879 · linkedin.com/in/kedarprabhu16/ · github.com/kedarsprabhu \\n \\nWork Experience \\nTricon Infotech – Bengaluru, India \\nSoftware Engineer Sept 2022-Present \\n• Implemented enhancements to an AI-enabled employee portal, introducing support for tables, \\nimages, graphs, and text data for Retrieval Augmented Generation tasks, while significantly \\nreducing operational costs by migrating from GPT to AWS-hosted models, resulting in a 95% \\ndecrease in input token costs and an 87.5% reduction in output token costs. Enabled compatibility \\nwith multiple file formats (e.g., PDF, ePUB, CSV) \\n• Enhanced information retrieval accuracy by implementing multi-vector retrieval architecture, \\nachieving 40% faster query response times and 35% improvement in search result relevance. \\n• Collaborated with a cross-functional team to develop new Angular UI for a learning management \\nplatform, improving search experience for educators and students. \\n• Developed high-performance FastAPIs with Uvicorn and asynchronous request handling, \\noptimizing API responsiveness and system scalability. \\n• Implemented intelligent agent orchestration using LangGraph, automating agent \\nselection and routing processes, significantly reducing manual user interactions while \\nimproving operational efficiency by 45%. \\nSkills \\n \\n• Python • Generative AI \\n• HTML • CSS \\n• JavaScript and TypeScript • Angular \\n• Structured Query Language \\n• Circle CI \\n• FastAPI \\n• AWS - Basics \\nEducation \\nPES Institute of Technology and Management, Shivamogga. Aug 2018- Aug 2022 \\nInformation Science Engineering 8.30 CGPA \\n \\nInternship Experience \\n \\nData Science Intern, Tata Consultancy Services. Aug 2021-Sep 2021 \\n• Built a forecasting system using ARIMA and Prophet time-series models to predict product \\ndemand and sales up to 2025 for a retail outlet based on historical data. \\nCertifications \\n• AWS Certified AI Practitioner  Jan 2025 \\n• Oracle Cloud Infrastructure 2024 Generative AI Professional July 2024 \\n• Introduction to Cloud Development with HTML, CSS, and JavaScript by edX. May 2021 \\n• Crash Course on Python by Google on Coursera. Nov 2020 \\nAchievements \\n• Played a key role in delivering and augmenting the AI library in the employee AI portal, \\naccelerating productivity and accurate responses for various use-cases, contributing to the team \\nbeing awarded \"Team of the Month.\" \\n• Contributed to the migration of all test suites from Protractor to Playwright with high code quality \\nand eventing logs, resulting in the team being recognized as \"Team of the Quarter.\" '), (UUID('341cf54e-1d1a-4505-b1e0-1f4fc56d573a'), '  \\nKEDAR SANTOSH PRABHU \\nkedarprabhu2000@gmail.com · +91-8310629879 · linkedin.com/in/kedarprabhu16/ · github.com/kedarsprabhu \\n \\nWork Experience \\nTricon Infotech – Bengaluru, India \\nSoftware Engineer Sept 2022-Present \\n• Implemented enhancements to an AI-enabled employee portal, introducing support for tables, \\nimages, graphs, and text data for Retrieval Augmented Generation tasks, while significantly \\nreducing operational costs by migrating from GPT to AWS-hosted models, resulting in a 95% \\ndecrease in input token costs and an 87.5% reduction in output token costs. Enabled compatibility \\nwith multiple file formats (e.g., PDF, ePUB, CSV) \\n• Enhanced information retrieval accuracy by implementing multi-vector retrieval architecture, \\nachieving 40% faster query response times and 35% improvement in search result relevance. \\n• Collaborated with a cross-functional team to develop new Angular UI for a learning management \\nplatform, improving search experience for educators and students. \\n• Developed high-performance FastAPIs with Uvicorn and asynchronous request handling, \\noptimizing API responsiveness and system scalability. \\n• Implemented intelligent agent orchestration using LangGraph, automating agent \\nselection and routing processes, significantly reducing manual user interactions while \\nimproving operational efficiency by 45%. \\nSkills \\n \\n• Python • Generative AI \\n• HTML • CSS \\n• JavaScript and TypeScript • Angular \\n• Structured Query Language \\n• Circle CI \\n• FastAPI \\n• AWS - Basics \\nEducation \\nPES Institute of Technology and Management, Shivamogga. Aug 2018- Aug 2022 \\nInformation Science Engineering 8.30 CGPA \\n \\nInternship Experience \\n \\nData Science Intern, Tata Consultancy Services. Aug 2021-Sep 2021 \\n• Built a forecasting system using ARIMA and Prophet time-series models to predict product \\ndemand and sales up to 2025 for a retail outlet based on historical data. \\nCertifications \\n• AWS Certified AI Practitioner  Jan 2025 \\n• Oracle Cloud Infrastructure 2024 Generative AI Professional July 2024 \\n• Introduction to Cloud Development with HTML, CSS, and JavaScript by edX. May 2021 \\n• Crash Course on Python by Google on Coursera. Nov 2020 \\nAchievements \\n• Played a key role in delivering and augmenting the AI library in the employee AI portal, \\naccelerating productivity and accurate responses for various use-cases, contributing to the team \\nbeing awarded \"Team of the Month.\" \\n• Contributed to the migration of all test suites from Protractor to Playwright with high code quality \\nand eventing logs, resulting in the team being recognized as \"Team of the Quarter.\" ')]\n"
     ]
    }
   ],
   "source": [
    "import resume_scanner\n",
    "\n",
    "all_candidates = resume_scanner.get_all_candidates()\n",
    "print(all_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CandidateMatch(BaseModel):\n",
    "    \"\"\"Model for candidate match analysis.\"\"\"\n",
    "    summary: str = Field(description=\"5-6 sentence summary of how well the candidate matches the job\")\n",
    "    match_percentage: float = Field(description=\"A percentage (0-100) indicating how well the candidate matches the job\")\n",
    "\n",
    "def summarize_candidate(resume_text, job_description, groq_api_key=None):\n",
    "    if groq_api_key is None:\n",
    "        groq_api_key = groq_api_key\n",
    "        if not groq_api_key:\n",
    "            raise ValueError(\"GROQ API key not found\")\n",
    "    # Create a parser for the Pydantic model\n",
    "    parser = PydanticOutputParser(pydantic_object=CandidateMatch)\n",
    "    \n",
    "    # Create a prompt template\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    As a Talent Acquisition AI, analyze how well this candidate matches the given job description.\n",
    "\n",
    "    **Candidate Resume:**  \n",
    "    {resume_text}\n",
    "\n",
    "    **Job Description:**  \n",
    "    {job_description}\n",
    "\n",
    "    Based on the candidate's skills, experience, and qualifications compared to the job requirements,\n",
    "    provide a concise summary (3-5 sentences) and calculate a match percentage (0-100%).\n",
    "\n",
    "    {format_instructions}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Set up the model\n",
    "    model = ChatGroq(\n",
    "        temperature=0.2, \n",
    "        groq_api_key=groq_api_key, \n",
    "        model_name=\"llama-3.1-8b-instant\"\n",
    "    )\n",
    "    \n",
    "    # Create the chain with the Pydantic parser\n",
    "    chain = prompt | model | parser\n",
    "    \n",
    "    # Invoke the chain\n",
    "    result = chain.invoke({\n",
    "        \"resume_text\": resume_text,\n",
    "        \"job_description\": job_description,\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }, config={\"callbacks\": [langfuse_handler]})\n",
    "    print(\"result\",result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9b2619cc-7b07-4db9-8d87-44bb9737f750\n",
      "result summary='The candidate, Kedar Santosh Prabhu, matches the job description well, with a strong background in Python, Generative AI, and full-stack development. He has experience with FastAPI, Angular, and AWS cloud infrastructure, which aligns with the job requirements. Additionally, he has implemented intelligent agent orchestration and retrieval augmented generation systems, and has optimized cloud-based AI model deployments. However, he lacks experience with time-series forecasting models and cloud AI model deployment and optimization. Overall, his skills and experience make him a strong candidate for the position.' match_percentage=85.0\n",
      "341cf54e-1d1a-4505-b1e0-1f4fc56d573a\n",
      "result summary='The candidate, Kedar Prabhu, is a strong match for the Software Engineer position. He has extensive experience in Generative AI, full-stack development, and cloud technologies, with a strong background in Python, AI/ML technologies, and web development frameworks. His skills and experience align closely with the job requirements, including design and implementation of AI-enabled enterprise platforms, high-performance API development, and cloud-based AI model deployments. He also has experience with LangGraph for agent orchestration, multi-vector retrieval architectures, and time-series forecasting models. Overall, his qualifications and experience make him a highly suitable candidate for the position.' match_percentage=92.0\n"
     ]
    }
   ],
   "source": [
    "candidate_matches = []\n",
    "groq_api_key = os.environ['GROQ_API_KEY']\n",
    "job_description = get_job_description('a49181cc-39a7-4308-877e-bf9c97e4af8c')\n",
    "for i, (candidate_id, resume_text) in enumerate(all_candidates):\n",
    "    \n",
    "    try:\n",
    "        print(candidate_id)\n",
    "        match_result = resume_scanner.summarize_candidate(resume_text, job_description, groq_api_key)\n",
    "        # print(f\"res{match_result}\")\n",
    "        candidate_matches.append({\n",
    "            \"candidate_id\": candidate_id,\n",
    "            \"resume_text\": resume_text,\n",
    "            \"match_percentage\": match_result.match_percentage,\n",
    "            \"summary\": match_result.summary\n",
    "        })\n",
    "        candidate_matches\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KEDAR SANTOSH PRABHU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_candidate_name(resume_text):\n",
    "    # Get the first line of the resume which often contains the name\n",
    "    first_line = resume_text.strip().split('\\n')[0]\n",
    "    # Split by common separators and take the first 1-3 words\n",
    "    words = first_line.split()\n",
    "    # Take first 1-3 words depending on what's available\n",
    "    if len(words) >= 3:\n",
    "        # print(' '.join(words[:3]).upper())\n",
    "        return ' '.join(words[:3]).upper()\n",
    "    elif len(words) >= 1:\n",
    "        # print(' '.join(words[:len(words)]).upper())\n",
    "        return ' '.join(words[:len(words)]).upper()\n",
    "    else:\n",
    "        return \"UNNAMED CANDIDATE\"  # Fallback\n",
    "    \n",
    "extract_candidate_name(\"KEDAR SANTOSH PRABHU kedarprabhu2000@gmail.com · +91-8310629879 · linkedin.com/in/kedarprabhu16/ · github.com/kedarsprabhu Work Experience Tricon Infotech – Bengaluru, India Software Engineer Sept 2022-Present • Implemented enhancements to an AI-enabled employee portal, introducing support for tables, images, graphs, and text data for Retrieval \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
